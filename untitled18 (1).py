# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VA99NpPNVevnFWzaRKy5necfomtK3qnd
"""

import tensorflow as tf### models
import pandas as pd ### reading and processing data
import seaborn as sns ### visualization
import numpy as np### math computations
import matplotlib.pyplot as plt### plotting bar chart
from tensorflow.keras.layers import Normalization, Dense, InputLayer
from tensorflow.keras.losses import MeanSquaredError, Huber, MeanAbsoluteError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam

data = pd.read_csv("/content/train.csv")
data.head()

data.shape

sns.pairplot(data[['years', 'km', 'rating', 'condition', 'economy', 'top speed', 'hp', 'torque', 'current price']], diag_kind='kde')

tensor_data = tf.constant(data)
tensor_data = tf.cast(tensor_data, tf.float32)
print(tensor_data)

tensor_data = tf.random.shuffle(tensor_data)
print(tensor_data[:5])

X = tensor_data[:,3:-1]
print(X[:5])

y = tensor_data[:,-1]
print(y[:5].shape)
y = tf.expand_dims(y, axis = -1)
print(y[:5])

print(X.shape)  # Should be (num_samples, num_features)
print(y.shape)  # Should be (num_samples,)

nor = Normalization()
x_nor = tf.constant([[1,2,3],
                     [3,4,5]])
nor.adapt(x_nor)
nor(x_nor)

nor = Normalization()
nor.adapt(X)
nor(X)



model = tf.keras.Sequential([

  nor,
  Dense(1)
],name="firstapi")

"""ye ek simple layer add kari hai isme output varibale 1 he hai and nor ka matlab hai normalizer
ab iusme input isliyeb mention nahi kara bcoz point is that nor layer uska apne aap dhayan rakh lega
"""

model = tf.keras.Sequential()

model.add(nor)

model.add(Dense(1))

"""ye bhi tarik hai add karne ka"""

tf.keras.utils.plot_model(
    model,
    to_file="my_first_ji.png",
    show_shapes = True,
)

modell = tf.keras.Sequential()

modell.add( InputLayer(input_shape=(8,)))

modell.add(nor)

modell.add(Dense(1))

tf.keras.utils.plot_model(
    model,
    to_file="myfirst_ji.png",
    show_shapes = True,
)

model = tf.keras.Sequential([
  InputLayer(input_shape=(8,)),
  nor,
  Dense(1)
],name="firstapii")

tf.keras.utils.plot_model(
    modell,
    to_file="myfirnst_ji.png",
    show_shapes = True,
)

tf.keras.utils.plot_model(
    model,
    to_file="myfirst_ji.png",
    show_shapes = True,
)

y = [1,2,4]
x = [3,1,4]
mse = tf.keras.losses.MeanSquaredError()
print(mse(x,y).numpy())

model.compile(optimizer='sgd',loss=tf.keras.losses.Huber())

"""n Keras, the verbose parameter is used to control the amount of information displayed during training of a model. It is commonly used in methods like fit(), evaluate(), and predict(). The verbose parameter can take the following values:

0: Silent mode. No output will be displayed during training.
1: Progress bar. A progress bar will be shown during training, which displays the epoch number, the percentage of completion, and other metrics like loss and accuracy.
2: One line per epoch. Instead of a progress bar, detailed information about the training progress will be shown on a per-epoch basis, including loss and accuracy metrics.

"""

X = tensor_data[:,3:-1]
print(X[:5])

y = tensor_data[:,-1]
print(y[:5].shape)
y = tf.expand_dims(y, axis = -1)
print(y[:5])

print(X.shape)  # Should be (num_samples, num_features)
print(y.shape)  # Should be (num_samples,)

nor = Normalization()
nor.adapt(X)
nor(X)

model.fit(X,y,epochs=10,verbose=1)

model.compile(optimizer=Adam(learning_rate=1e-3),loss=tf.keras.losses.Huber())

histoy = model.fit(X,y,epochs=100,verbose=1)

histoy.history

histoy.history["loss"]

plt.plot(histoy.history["loss"])
plt.title("LOSS in Model")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.show()

model.compile(optimizer=Adam(learning_rate=1e-3),loss=tf.keras.losses.Huber(),metrics=RootMeanSquaredError())

model.fit(X,y,epochs=10,verbose=1)

model.evaluate(X,y)

TRAIN_RATIO = 0.8
VAL_RATIO = 0.1
TEST_RATIO = 0.1
DATA_SIZE = len(X)

X_train = X[:int(TRAIN_RATIO*DATA_SIZE )]

y_train = y[:int(TRAIN_RATIO*DATA_SIZE)]

print(X_train.shape)

print(y_train.shape)



X_val = X[int(TRAIN_RATIO*DATA_SIZE ):int((TRAIN_RATIO+VAL_RATIO)*DATA_SIZE )]

y_val = y[int(TRAIN_RATIO*DATA_SIZE ):int((TRAIN_RATIO+VAL_RATIO)*DATA_SIZE )]

print(X_val.shape)

print(y_val.shape)

X_test = X[int((TRAIN_RATIO+VAL_RATIO)*DATA_SIZE ):]
y_test = y[int((TRAIN_RATIO+VAL_RATIO)*DATA_SIZE ):]
print(X_test.shape,y_test.shape)

normalizer = Normalization()
normalizer.adapt(X_test)

histo = model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=100,verbose=1)

histo = model.fit(X,y,validation_split=0.2,epochs=100,verbose=1)

plt.plot(histo.history['loss'])
plt.plot(histo.history['val_loss'])
plt.title('model loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val_loss'])
plt.show()

y_predict = model(X_test)[:,0].numpy()

print(y_predict.shape)

y_true = y_test[:,0].numpy()
print(y_true)

ind = np.arange(100)
plt.figure(figsize=(40,20))

width = 0.1

plt.bar(ind, y_predict, width, label='Predicted Car Price')
plt.bar(ind + width, y_true, width, label='Actual Car Price')

plt.xlabel('Actual vs Predicted Prices')
plt.ylabel('Car Price Prices')

plt.show()

"""figsize=(40,20) is an argument that specifies the size of the figure in inches. The first value 40 represents the width and the second value 20 represents the height.

our model is underperforming as dffference between acutual and predicted is very high

Imp : ye samajhna hai hme ki hamare model mai hmne validation and tarining data set pai bure perform kara
matalab ye condition underfitting ki hai atleast model ko training data pai toh sahi perform kara chahiye


ab iske do  conclusion nikalte hai ya toh model chosen he galt hai ya hmne bahut simple model banaya hai

ab ham isko complex karenge bcoz after evaluating we got to understand that 9 faetures are not enough to learn the trating
data of 1000 data set  so for doing that we would add hidden more layers
"""



model = tf.keras.Sequential(
    [
        normalizer,
        Dense(128 , activation='relu'),
        Dense(128 , activation='relu'),
            Dense(28 , activation='relu'),
        Dense(1)
    ])

model.compile(optimizer=Adam(learning_rate=1e-3),loss=tf.keras.losses.Huber(),metrics=RootMeanSquaredError())



"""iska matlab overfitting ho gayi hai"""

histo = model.fit(X,y,validation_split=0.2,epochs=200,verbose=1)

plt.plot(histo.history['loss'])
plt.plot(histo.history['val_loss'])
plt.title('model loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val_loss'])
plt.show()

"""ab ja ke sahi aya jab main no of dense layer badai

"""

BUFFER_SIZE = 16
BATCH_SIZE =  64
train_dataset = tf.data.Dataset.from_tensor_slices((X_train,y_train))
train_dataset = train_dataset.shuffle(buffer_size = BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

"""jahan pair dalte the uske jagah ye kam a jayga bs"""